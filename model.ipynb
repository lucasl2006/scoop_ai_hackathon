{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7518e0-2433-42b2-8b19-bb7fbb2f42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Linear(10, 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Optimizer works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e548a9b5-7dbd-48c9-8154-4bec10e191ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 640)\n",
      "   0    1    2    3    4    5            6    7    8    9    ...  630  631  \\\n",
      "0  1.0    1    1    1    1    1  75712099.05    1    1    1  ...  0.0  0.0   \n",
      "1  1.0    1    1    1    1    1  47189363.60    1    1    1  ...  NaN  NaN   \n",
      "2  0.0    1    1    1    1    1  19763851.20    1    1    1  ...  NaN  NaN   \n",
      "3  1.0    1    1    1    1    1  12688293.50    1    1    1  ...  NaN  NaN   \n",
      "4  1.0    1    1    1    1    1  12056225.30    1    1    1  ...  NaN  NaN   \n",
      "\n",
      "   632  633  634  635  636     637   638  639  \n",
      "0  0.0  0.0  0.0  0.0  0.0  402.43  42.0  1.0  \n",
      "1  NaN  NaN  NaN  NaN  NaN     NaN   NaN  NaN  \n",
      "2  NaN  NaN  NaN  NaN  NaN     NaN   NaN  NaN  \n",
      "3  NaN  NaN  NaN  NaN  NaN     NaN   NaN  NaN  \n",
      "4  NaN  NaN  NaN  NaN  NaN     NaN   NaN  NaN  \n",
      "\n",
      "[5 rows x 640 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV (assuming no headers, just raw numeric rows)\n",
    "df = pd.read_csv(\"./data/schitt.csv\", header=None)\n",
    "\n",
    "print(df.shape)   # rows x columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea539dd-472d-433c-9d20-0747f76bf697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "(97, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "MAX_LEN = 640\n",
    "\n",
    "# Convert each row to numpy array and pad\n",
    "data = []\n",
    "\n",
    "for row in df.values:\n",
    "    # Convert all items to numeric (invalid → NaN)\n",
    "    row = pd.to_numeric(row, errors='coerce')\n",
    "\n",
    "    # Drop NaNs\n",
    "    row = row[~np.isnan(row)]\n",
    "\n",
    "    # Convert to float32\n",
    "    row = row.astype(np.float32)\n",
    "\n",
    "    # Pad\n",
    "    if len(row) < MAX_LEN:\n",
    "        padded = np.pad(row, (0, MAX_LEN - len(row)), constant_values=0)\n",
    "    else:\n",
    "        padded = row[:MAX_LEN]\n",
    "\n",
    "    data.append(padded)\n",
    "\n",
    "data = np.array(data, dtype=np.float32)\n",
    "\n",
    "# Remove any leftover inf/NaN\n",
    "data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(np.isnan(data).any())   # should be False\n",
    "print(np.isinf(data).any())   # should be False\n",
    "\n",
    "print(data.shape)  # (num_samples, 640)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Example: last column is label, rest are features\n",
    "# First column is the label\n",
    "y = data[:, 0].astype(int)   # labels (0 or 1)\n",
    "X = data[:, 1:]              # features (639 columns after padding)\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e491e2c-8f54-4eb1-af5b-fa4e87e2196d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad20bf18-f5c8-4613-af23-9e1b55adcfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7406\n",
      "Epoch 2, Loss: 0.6004\n",
      "Epoch 3, Loss: 0.5878\n",
      "Epoch 4, Loss: 0.3906\n",
      "Epoch 5, Loss: 0.5622\n",
      "Epoch 6, Loss: 0.4774\n",
      "Epoch 7, Loss: 0.4623\n",
      "Epoch 8, Loss: 0.4389\n",
      "Epoch 9, Loss: 0.4883\n",
      "Epoch 10, Loss: 0.3967\n",
      "Epoch 11, Loss: 0.3428\n",
      "Epoch 12, Loss: 0.2616\n",
      "Epoch 13, Loss: 0.3517\n",
      "Epoch 14, Loss: 0.3193\n",
      "Epoch 15, Loss: 0.2558\n",
      "Epoch 16, Loss: 0.2779\n",
      "Epoch 17, Loss: 0.2738\n",
      "Epoch 18, Loss: 0.2170\n",
      "Epoch 19, Loss: 0.3243\n",
      "Epoch 20, Loss: 0.2961\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Wrap in DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim=639, hidden_dim=128, output_dim=2):  # <-- 2 classes\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleModel(input_dim=639, hidden_dim=128, output_dim=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):  # 10 epochs\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242f9ee0-bb80-41c9-8ce2-9bf4b82da093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [1]\n",
      "Max feature value: 8.69541e+07\n",
      "Min feature value: 0.0\n",
      "Any nonzero features: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique labels:\", np.unique(y))\n",
    "print(\"Max feature value:\", np.max(X))\n",
    "print(\"Min feature value:\", np.min(X))\n",
    "print(\"Any nonzero features:\", np.any(X != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94a1ee7c-e7c4-42bc-8450-6e54ab1fddf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m optimizer.zero_grad()\n\u001b[32m      5\u001b[39m outputs = model(X_batch)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m loss.backward()\n\u001b[32m      8\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3_13\\envs\\ai_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3_13\\envs\\ai_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3_13\\envs\\ai_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3_13\\envs\\ai_env\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52748b91-7be8-4cb7-a817-34ad9c38cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 chance: 0.00%\n",
      "Class 1 chance: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LEN = 640       # same as before\n",
    "FEATURES = 639      # model input size\n",
    "\n",
    "# Load your 1-row CSV (no label column)\n",
    "row = pd.read_csv(\"./data/wayfair_one.csv\", header=None).values.flatten()\n",
    "\n",
    "# Convert to numeric\n",
    "row = pd.to_numeric(row, errors='coerce')\n",
    "\n",
    "# Remove NaNs\n",
    "row = row[~np.isnan(row)]\n",
    "\n",
    "# Convert to float32\n",
    "row = row.astype(np.float32)\n",
    "row = np.nan_to_num(row, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Pad to 639 features + 1 label position (but label is missing)\n",
    "# Training format = [label | 639 features]\n",
    "# Here we only have the 639 features, so if row is shorter → pad to 639\n",
    "if len(row) < FEATURES:\n",
    "    row = np.pad(row, (0, FEATURES - len(row)), constant_values=0)\n",
    "\n",
    "row = scaler.transform([row])  # keep 2D shape\n",
    "tensor_row = torch.tensor(row, dtype=torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(tensor_row)           # shape: [1, 2] for 2 classes\n",
    "    probs = F.softmax(output, dim=1)\n",
    "\n",
    "percent_class_0 = probs[0][0].item() * 100\n",
    "percent_class_1 = probs[0][1].item() * 100\n",
    "\n",
    "print(f\"Class 0 chance: {percent_class_0:.2f}%\")\n",
    "print(f\"Class 1 chance: {percent_class_1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c8f01-f572-47a6-99bc-62a5c6eab2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
